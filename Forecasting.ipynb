{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "477f5cf1",
   "metadata": {},
   "source": [
    "# Time Series Diagnostics, Modeling & Forecast — Monthly TB Pulmonary\n",
    "# EN: Time series diagnostics, SARIMA / ETS model comparison, and forecast (train: 2001-2009, test: 2010, forecast: 2011).\n",
    "# PT-BR: Diagnóstico de séries temporais, comparação SARIMA / ETS e previsão (treino: 2001-2009, teste: 2010, previsão: 2011).\n",
    "\n",
    "**EN:** This notebook builds candidate time-series models for the monthly pulmonary TB series, performs backtesting\n",
    "on 2010, compares metrics, and produces a final forecast for 2011 with prediction intervals.\n",
    "\n",
    "**PT-BR:** Este notebook constrói modelos candidatos para a série mensal de TB pulmonar, faz backtesting em 2010,\n",
    "compara métricas e gera previsão final para 2011 com intervalos de confiança."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ece76c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EN/PT: Setup imports, flags, paths, plotting defaults\n",
    "DEBUG = True\n",
    "LANGUAGE = \"EN\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "# Plot defaults\n",
    "plt.rcParams.update({\"figure.figsize\": (12, 5), \"font.size\": 12})\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "DATA_DIR = Path(\"processed_data\")\n",
    "FIG_DIR = Path(\"figures\")\n",
    "FIG_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "def debug(en_msg, pt_msg=\"\"):\n",
    "    if not DEBUG:\n",
    "        return\n",
    "    print(\"[DEBUG]\", en_msg if LANGUAGE.upper()==\"EN\" else (pt_msg if pt_msg else en_msg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb97594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EN/PT: Load monthly series CSV produced by preprocessing. Expect columns: date (or date_parsed) and cases\n",
    "fn_monthly = DATA_DIR / \"monthly_pulmonary_tb_by_month.csv\"\n",
    "if not fn_monthly.exists():\n",
    "    raise FileNotFoundError(f\"Monthly CSV not found: {fn_monthly} — run preprocessing first. / CSV mensal não encontrado: {fn_monthly} — rode o pré-processamento primeiro.\")\n",
    "\n",
    "df = pd.read_csv(fn_monthly, parse_dates=[\"date\"] if \"date\" in pd.read_csv(fn_monthly, nrows=0).columns else None, low_memory=False)\n",
    "\n",
    "# Ensure date column exists (try date_parsed fallback)\n",
    "if \"date\" not in df.columns and \"date_parsed\" in df.columns:\n",
    "    df = df.rename(columns={\"date_parsed\":\"date\"})\n",
    "\n",
    "if \"date\" not in df.columns:\n",
    "    # try build from year/month\n",
    "    if \"year\" in df.columns and \"month\" in df.columns:\n",
    "        df[\"date\"] = pd.to_datetime(df[\"year\"].astype(int).astype(str) + \"-\" + df[\"month\"].astype(int).astype(str) + \"-01\", errors=\"coerce\")\n",
    "    else:\n",
    "        raise SystemExit(\"No 'date' column and no year/month to construct it. / Sem coluna 'date' nem year/month para construir.\")\n",
    "\n",
    "df = df.sort_values(\"date\").reset_index(drop=True)\n",
    "df = df[[\"date\",\"cases\"]].copy()\n",
    "df[\"cases\"] = pd.to_numeric(df[\"cases\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "# Keep only up to 2010-12-31 (if dataset includes later)\n",
    "cutoff = pd.to_datetime(\"2010-12-31\")\n",
    "df = df[df[\"date\"] <= cutoff].copy()\n",
    "\n",
    "# Set index\n",
    "df = df.set_index(\"date\").asfreq(\"MS\")  # monthly start frequency; missing months will be NaN\n",
    "series = df[\"cases\"].fillna(0)  # for modeling we may need to handle NaNs; temporarily fill 0 for display\n",
    "\n",
    "debug(f\"Loaded monthly series with {len(series)} points from {series.index.min().date()} to {series.index.max().date()}\",\n",
    "    f\"Série mensal carregada com {len(series)} pontos de {series.index.min().date()} a {series.index.max().date()}\")\n",
    "\n",
    "display(series.head(24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bccd170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EN/PT: STL decomposition and ACF/PACF plots\n",
    "stl = STL(series, period=12, robust=True)\n",
    "res = stl.fit()\n",
    "fig = res.plot()\n",
    "fig.set_size_inches(12,8)\n",
    "stl_path = FIG_DIR / \"stl_sarima_diagnostics.png\"\n",
    "fig.savefig(stl_path, bbox_inches=\"tight\", dpi=150)\n",
    "dbg(f\"Saved STL decomposition: {stl_path}\", f\"STL salvo: {stl_path}\")\n",
    "plt.show()\n",
    "\n",
    "# ACF/PACF\n",
    "plt.figure(figsize=(12,4))\n",
    "plot_acf(series, lags=36)\n",
    "acf_path = FIG_DIR / \"acf_for_model.png\"\n",
    "plt.savefig(acf_path, bbox_inches=\"tight\", dpi=150)\n",
    "dbg(f\"Saved ACF plot: {acf_path}\", f\"ACF salvo: {acf_path}\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plot_pacf(series, lags=36, method='ywm')\n",
    "pacf_path = FIG_DIR / \"pacf_for_model.png\"\n",
    "plt.savefig(pacf_path, bbox_inches=\"tight\", dpi=150)\n",
    "dbg(f\"Saved PACF plot: {pacf_path}\", f\"PACF salvo: {pacf_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b533ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EN/PT: Run ADF and KPSS tests to check stationarity\n",
    "def adf_test(x):\n",
    "    r = adfuller(x.dropna(), autolag='AIC')\n",
    "    return {\"adf_stat\": r[0], \"pvalue\": r[1], \"usedlag\": r[2], \"nobs\": r[3]}\n",
    "\n",
    "def kpss_test(x):\n",
    "    r = kpss(x.dropna(), nlags=\"auto\")\n",
    "    return {\"kpss_stat\": r[0], \"pvalue\": r[1], \"nlags\": r[2]}\n",
    "\n",
    "adf_res = adf_test(series)\n",
    "kpss_res = kpss_test(series)\n",
    "\n",
    "dbg(f\"ADF: {adf_res}\", f\"ADF: {adf_res}\")\n",
    "dbg(f\"KPSS: {kpss_res}\", f\"KPSS: {kpss_res}\")\n",
    "\n",
    "print(\"ADF p-value (stationary if < 0.05):\", adf_res[\"pvalue\"])\n",
    "print(\"KPSS p-value (stationary if > 0.05):\", kpss_res[\"pvalue\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bea9a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EN/PT: Train/test split: train <= 2009-12-01, test = 2010-01..2010-12\n",
    "train_end = pd.to_datetime(\"2009-12-01\")\n",
    "test_start = pd.to_datetime(\"2010-01-01\")\n",
    "test_end = pd.to_datetime(\"2010-12-01\")\n",
    "\n",
    "train = series[series.index <= train_end].copy()\n",
    "test = series[(series.index >= test_start) & (series.index <= test_end)].copy()\n",
    "\n",
    "debug(f\"Train points: {len(train)}, Test points: {len(test)}\", f\"Treino: {len(train)}, Teste: {len(test)}\")\n",
    "display(train.tail(6))\n",
    "display(test.head(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f52d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EN/PT: Define evaluation metrics\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    # avoid division by zero: mask zeros\n",
    "    mask = y_true != 0\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d63e057",
   "metadata": {},
   "source": [
    "## Modeling strategy / Estratégia de modelagem\n",
    "\n",
    "**EN:** We'll fit two families of models:\n",
    "- SARIMA via statsmodels (seasonal order with s=12). We'll perform a small grid search over (p,d,q) x (P,D,Q) limited ranges to keep runtime reasonable.\n",
    "- ETS (Holt-Winters) via statsmodels' ExponentialSmoothing as a benchmark.\n",
    "\n",
    "We will evaluate on the 2010 holdout and choose the best model by RMSE/MAE/MAPE.\n",
    "\n",
    "**PT-BR:** Ajustaremos duas famílias:\n",
    "- SARIMA (statsmodels) com busca em grade limitada para (p,d,q)x(P,D,Q) com s=12.\n",
    "- ETS (Holt-Winters) como benchmark.\n",
    "\n",
    "Avaliaremos no conjunto de 2010 e escolheremos pelo RMSE/MAE/MAPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e67c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EN/PT: SARIMA grid search (limited) — careful with runtime\n",
    "# ranges chosen small to be practical\n",
    "ps = [0,1,2]\n",
    "ds = [0,1]\n",
    "qs = [0,1,2]\n",
    "Ps = [0,1]\n",
    "Ds = [0,1]\n",
    "Qs = [0,1]\n",
    "s = 12\n",
    "\n",
    "best_sarima = None\n",
    "results = []\n",
    "start_time = time.time()\n",
    "\n",
    "# We'll fit SARIMA on TRAIN and forecast the 12 months of TEST, compute RMSE\n",
    "for p,d,q in itertools.product(ps,ds,qs):\n",
    "    for P,D,Q in itertools.product(Ps,Ds,Qs):\n",
    "        order = (p,d,q)\n",
    "        seasonal_order = (P,D,Q,s)\n",
    "        try:\n",
    "            model = SARIMAX(train, order=order, seasonal_order=seasonal_order, enforce_stationarity=False, enforce_invertibility=False)\n",
    "            res = model.fit(disp=False, method=\"lbfgs\", maxiter=200)\n",
    "            # forecast for test period\n",
    "            pred = res.get_forecast(steps=len(test))\n",
    "            yhat = pred.predicted_mean\n",
    "            score_rmse = rmse(test.values, yhat.values)\n",
    "            score_mae = mae(test.values, yhat.values)\n",
    "            score_mape = mape(test.values, yhat.values)\n",
    "            results.append({\"order\":order, \"seasonal_order\":seasonal_order, \"rmse\":score_rmse, \"mae\":score_mae, \"mape\":score_mape, \"aic\":res.aic})\n",
    "            debug(f\"SARIMA tried order={order} seasonal={seasonal_order} RMSE={score_rmse:.2f}\", f\"SARIMA testado order={order} seasonal={seasonal_order} RMSE={score_rmse:.2f}\")\n",
    "        except Exception as e:\n",
    "            debug(f\"SARIMA failed for order={order} seasonal={seasonal_order}: {e}\", f\"SARIMA falhou para order={order} seasonal={seasonal_order}: {e}\")\n",
    "            continue\n",
    "\n",
    "# collect results DataFrame\n",
    "df_sarima_results = pd.DataFrame(results).sort_values(\"rmse\").reset_index(drop=True)\n",
    "debug(f\"SARIMA grid search completed in {time.time()-start_time:.1f}s; best candidate: {df_sarima_results.iloc[0].to_dict() if not df_sarima_results.empty else 'none'}\",\n",
    "    f\"Busca SARIMA concluída em {time.time()-start_time:.1f}s; melhor candidato: {df_sarima_results.iloc[0].to_dict() if not df_sarima_results.empty else 'nenhum'}\")\n",
    "\n",
    "display(df_sarima_results.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ef0062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EN/PT: Fit the best SARIMA from grid search on full TRAIN data (retrain) and produce forecast for 2011 (12 months)\n",
    "if df_sarima_results.empty:\n",
    "    dbg(\"No SARIMA candidate found — skipping SARIMA\", \"Nenhum candidato SARIMA encontrado — pulando SARIMA\")\n",
    "else:\n",
    "    best = df_sarima_results.iloc[0]\n",
    "    best_order = tuple(best[\"order\"])\n",
    "    best_seasonal = tuple(best[\"seasonal_order\"])\n",
    "    dbg(f\"Refitting best SARIMA order={best_order} seasonal={best_seasonal}\", f\"Refit do melhor SARIMA order={best_order} seasonal={best_seasonal}\")\n",
    "\n",
    "    sarima_model = SARIMAX(train, order=best_order, seasonal_order=best_seasonal, enforce_stationarity=False, enforce_invertibility=False)\n",
    "    sarima_res = sarima_model.fit(disp=False, method=\"lbfgs\", maxiter=500)\n",
    "    # forecast for test (again, to show fitted vs actual)\n",
    "    sarima_pred_test = sarima_res.get_forecast(steps=len(test))\n",
    "    sarima_pred_mean_test = sarima_pred_test.predicted_mean\n",
    "    sarima_pred_ci_test = sarima_pred_test.conf_int(alpha=0.05)\n",
    "\n",
    "    # Forecast 12 months for 2011 (we'll do this after evaluating)\n",
    "    sarima_forecast_2011 = sarima_res.get_forecast(steps=12)\n",
    "    sarima_forecast_mean = sarima_forecast_2011.predicted_mean\n",
    "    sarima_forecast_ci = sarima_forecast_2011.conf_int(alpha=0.05)\n",
    "\n",
    "    # Save model using pickle\n",
    "    import pickle\n",
    "    with open(DATA_DIR / \"sarima_model.pkl\", \"wb\") as f:\n",
    "        pickle.dump(sarima_res, f)\n",
    "\n",
    "    dbg(\"SARIMA model refit and saved to processed_data/sarima_model.pkl\",\n",
    "        \"SARIMA refitado e salvo em processed_data/sarima_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a47a972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EN/PT: Fit ETS (Holt-Winters) on TRAIN and forecast\n",
    "# We'll try additive trend + additive seasonality as baseline; user can tune\n",
    "try:\n",
    "    ets = ExponentialSmoothing(train, trend=\"add\", seasonal=\"add\", seasonal_periods=12, initialization_method=\"estimated\")\n",
    "    ets_res = ets.fit(optimized=True)\n",
    "    ets_pred_test = ets_res.forecast(steps=len(test))\n",
    "    ets_forecast_2011 = ets_res.forecast(steps=12)  # forecast for 2011\n",
    "    # Save ETS model\n",
    "    import pickle\n",
    "    with open(DATA_DIR / \"ets_model.pkl\", \"wb\") as f:\n",
    "        pickle.dump(ets_res, f)\n",
    "    dbg(\"ETS model trained and saved\", \"ETS treinado e salvo\")\n",
    "except Exception as e:\n",
    "    dbg(f\"ETS fitting failed: {e}\", f\"Falha no ajuste ETS: {e}\")\n",
    "    ets_res = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7527d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EN/PT: Evaluate SARIMA and ETS on the 2010 test set\n",
    "eval_rows = []\n",
    "if not df_sarima_results.empty:\n",
    "    sarima_yhat = sarima_pred_mean_test\n",
    "    eval_rows.append({\n",
    "        \"model\":\"SARIMA\",\n",
    "        \"rmse\": rmse(test.values, sarima_yhat.values),\n",
    "        \"mae\": mae(test.values, sarima_yhat.values),\n",
    "        \"mape\": mape(test.values, sarima_yhat.values)\n",
    "    })\n",
    "\n",
    "if ets_res is not None:\n",
    "    ets_yhat = ets_pred_test\n",
    "    eval_rows.append({\n",
    "        \"model\":\"ETS\",\n",
    "        \"rmse\": rmse(test.values, ets_yhat.values),\n",
    "        \"mae\": mae(test.values, ets_yhat.values),\n",
    "        \"mape\": mape(test.values, ets_yhat.values)\n",
    "    })\n",
    "\n",
    "df_eval = pd.DataFrame(eval_rows).sort_values(\"rmse\").reset_index(drop=True)\n",
    "display(df_eval)\n",
    "df_eval.to_csv(DATA_DIR / \"model_evaluation_on_2010.csv\", index=False)\n",
    "dbg(\"Saved evaluation table to processed_data/model_evaluation_on_2010.csv\",\n",
    "    \"Avaliação salva em processed_data/model_evaluation_on_2010.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d496a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EN/PT: Plot test period (2010) actual vs predictions (SARIMA and ETS)\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(train.index, train, label=\"Train\", color=\"black\")\n",
    "plt.plot(test.index, test, label=\"Actual 2010\", color=\"black\", linestyle=\"--\", marker='o')\n",
    "\n",
    "if not df_sarima_results.empty:\n",
    "    plt.plot(test.index, sarima_pred_mean_test, label=\"SARIMA pred (2010)\", marker='o')\n",
    "    ci = sarima_pred_ci_test\n",
    "    plt.fill_between(test.index, ci.iloc[:,0], ci.iloc[:,1], color='C0', alpha=0.2)\n",
    "\n",
    "if ets_res is not None:\n",
    "    plt.plot(test.index, ets_pred_test, label=\"ETS pred (2010)\", marker='o')\n",
    "\n",
    "plt.title(\"Model predictions vs actual (2010) / Predições vs observados (2010)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt_path = FIG_DIR / \"model_predictions_vs_actual_2010.png\"\n",
    "plt.savefig(plt_path, bbox_inches=\"tight\", dpi=150)\n",
    "dbg(f\"Saved prediction vs actual plot: {plt_path}\", f\"Plot predições vs observados salvo: {plt_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab29b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EN/PT: Produce and save final 2011 forecasts (SARIMA and ETS) with intervals where available\n",
    "forecasts = []\n",
    "\n",
    "if not df_sarima_results.empty:\n",
    "    sarima_fc = sarima_forecast_mean\n",
    "    sarima_ci = sarima_forecast_ci\n",
    "    sarima_df = pd.DataFrame({\n",
    "        \"date\": pd.date_range(start=\"2011-01-01\", periods=12, freq=\"MS\"),\n",
    "        \"forecast_sarima\": sarima_fc.values,\n",
    "        \"lower_sarima\": sarima_ci.iloc[:,0].values,\n",
    "        \"upper_sarima\": sarima_ci.iloc[:,1].values\n",
    "    })\n",
    "    forecasts.append(sarima_df)\n",
    "\n",
    "if ets_res is not None:\n",
    "    ets_fc = ets_forecast_2011\n",
    "    ets_df = pd.DataFrame({\n",
    "        \"date\": pd.date_range(start=\"2011-01-01\", periods=12, freq=\"MS\"),\n",
    "        \"forecast_ets\": ets_fc.values\n",
    "    })\n",
    "    forecasts.append(ets_df)\n",
    "\n",
    "# Merge forecasts if both exist\n",
    "if forecasts:\n",
    "    df_forecast = forecasts[0]\n",
    "    for f in forecasts[1:]:\n",
    "        df_forecast = df_forecast.merge(f, on=\"date\", how=\"outer\")\n",
    "    df_forecast.to_csv(DATA_DIR / \"forecast_2011_models.csv\", index=False)\n",
    "    dbg(\"Saved 2011 forecasts to processed_data/forecast_2011_models.csv\",\n",
    "        \"Previsões 2011 salvas em processed_data/forecast_2011_models.csv\")\n",
    "    display(df_forecast)\n",
    "else:\n",
    "    dbg(\"No forecasts produced.\", \"Nenhuma previsão produzida.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a91b5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EN/PT: Plot history (2001-2010) + forecasts for 2011 with intervals\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(series.index, series.values, label=\"History (2001-2010)\", color=\"black\")\n",
    "\n",
    "if not df_sarima_results.empty:\n",
    "    plt.plot(df_forecast[\"date\"], df_forecast[\"forecast_sarima\"], label=\"SARIMA 2011\", marker='o')\n",
    "    plt.fill_between(df_forecast[\"date\"], df_forecast[\"lower_sarima\"], df_forecast[\"upper_sarima\"], color='C0', alpha=0.15)\n",
    "\n",
    "if ets_res is not None:\n",
    "    plt.plot(df_forecast[\"date\"], df_forecast[\"forecast_ets\"], label=\"ETS 2011\", marker='o')\n",
    "\n",
    "plt.title(\"History + Forecast (2011) / Histórico + Previsão (2011)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt_path = FIG_DIR / \"history_plus_forecast_2011.png\"\n",
    "plt.savefig(plt_path, bbox_inches=\"tight\", dpi=150)\n",
    "dbg(f\"Saved history + forecast plot: {plt_path}\", f\"Histórico + previsão salvo: {plt_path}\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
