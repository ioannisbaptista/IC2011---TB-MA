{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ec2e97c",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "\n",
    "This script performs data checks, descriptive tables and the core visualizations required to understand the monthly Pulmonary Tuberculosis series (2001–2010) and to prepare inputs for time-series modeling and forecasting (2011 prediction)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d543c6",
   "metadata": {},
   "source": [
    "## Step #1: Initiate environment for **Exploratory Data Analysis**\n",
    "This section imports necessary libraries, creates debugging flags and control variables and configures paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089cc88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EN: Setup cell - imports, debug/lang flags, paths, and plotting defaults\n",
    "# PT-BR: Célula de setup - imports, flags de debug/idioma, caminhos e defaults de plot\n",
    "\n",
    "DEBUG = True        # EN: Show debug messages?  PT-BR: Mostrar mensagens de debug?\n",
    "LANGUAGE = \"EN\"     # EN: \"EN\" or \"PT\"  PT-BR: \"EN\" ou \"PT\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime, timedelta\n",
    "import locale\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import time\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "# Plot defaults\n",
    "plt.rcParams.update({\"figure.figsize\": (12, 5), \"font.size\": 12})\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Paths (adjust if needed)\n",
    "DATA_DIR = Path(\"processed_data\")\n",
    "FIG_DIR = Path(\"figures\")\n",
    "TAB_DIR = Path(\"tables\")\n",
    "FIG_DIR.mkdir(exist_ok=True, parents=True)\n",
    "TAB_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "def debug(en_msg, pt_msg=\"\"):\n",
    "    if not DEBUG:\n",
    "        return\n",
    "    msg = en_msg if LANGUAGE.upper()==\"EN\" else (pt_msg if pt_msg else en_msg)\n",
    "    print(\"[DEBUG]\", msg)\n",
    "\n",
    "debug(\"Setup completed. Data dir = processed_data/\", \"Setup completo. Pasta de dados = processed_data/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06681f9e",
   "metadata": {},
   "source": [
    "### Step 2: **Load Data**\n",
    "Load the preprocessed data for EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbc6974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EN: Load core CSVs (already preprocessed)\n",
    "# PT-BR: Carregar CSVs principais (já pré-processados)\n",
    "\n",
    "fn_filtered = DATA_DIR / \"filtered_pulmonary_tb_rows.csv\"\n",
    "fn_monthly = DATA_DIR / \"monthly_pulmonary_tb_by_month.csv\"\n",
    "fn_annual = DATA_DIR / \"annual_pulmonary_tb_by_year.csv\"\n",
    "\n",
    "for f in (fn_filtered, fn_monthly, fn_annual):\n",
    "    if not f.exists():\n",
    "        raise FileNotFoundError(f\"Required file not found: {f} — check pre-processing step. / Arquivo necessário não encontrado: {f} — verifique pré-processamento.\")\n",
    "\n",
    "df_raw = pd.read_csv(fn_filtered, low_memory=False)\n",
    "df_monthly = pd.read_csv(fn_monthly, parse_dates=[\"date\"] if \"date\" in pd.read_csv(fn_monthly, nrows=0).columns else None)\n",
    "df_annual = pd.read_csv(fn_annual)\n",
    "\n",
    "debug(f\"Loaded: filtered={len(df_raw)} rows, monthly={len(df_monthly)} rows, annual={len(df_annual)} rows\",\n",
    "    f\"Carregados: filtrados={len(df_raw)} linhas, mensal={len(df_monthly)} linhas, anual={len(df_annual)} linhas\")\n",
    "\n",
    "# Quick peek\n",
    "df_raw.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14376bcc",
   "metadata": {},
   "source": [
    "### Step 3: **Check Data**\n",
    "Check the loaded data for errors and inconsistencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae87f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EN: Quick sanity checks: dtypes, missing counts, date parse validation\n",
    "# PT-BR: Checagens rápidas: dtypes, contagem de missings, validação de data\n",
    "\n",
    "def print_section(title_en, title_pt):\n",
    "    if LANGUAGE.upper()==\"EN\":\n",
    "        print(title_en)\n",
    "    else:\n",
    "        print(title_pt)\n",
    "\n",
    "print_section(\"=== RAW ROWS: info() ===\", \"=== LINHAS FILTRADAS: info() ===\")\n",
    "display(df_raw.info())\n",
    "\n",
    "print_section(\"\\n=== MONTHLY: head() & dtypes ===\", \"\\n=== MENSAL: head() & tipos ===\")\n",
    "display(df_monthly.head())\n",
    "display(df_monthly.dtypes)\n",
    "\n",
    "print_section(\"\\n=== Missing values (raw) ===\", \"\\n=== Valores faltantes (raw) ===\")\n",
    "display(df_raw.isna().sum().to_frame(\"missing_count\").sort_values(\"missing_count\", ascending=False).head(40))\n",
    "\n",
    "print_section(\"\\n=== Missing values (monthly) ===\", \"\\n=== Valores faltantes (mensal) ===\")\n",
    "display(df_monthly.isna().sum().to_frame(\"missing_count\").sort_values(\"missing_count\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49ef342",
   "metadata": {},
   "source": [
    "### Step 4: **Check Data Types and Statistics**\n",
    "Check the loaded data for incompatible data types and extract basic statistics for initial investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a30b8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EN: Ensure 'date' is datetime and set monthly index; compute basic descriptive stats for monthly series\n",
    "# PT-BR: Garantir que 'date' é datetime e definir index mensal; calcular estatísticas descritivas básicas\n",
    "\n",
    "# Ensure date column:\n",
    "if \"date\" in df_monthly.columns:\n",
    "    df_monthly[\"date\"] = pd.to_datetime(df_monthly[\"date\"], errors=\"coerce\")\n",
    "else:\n",
    "    # fallback: construct date if year/month columns exist\n",
    "    if \"year\" in df_monthly.columns and \"month\" in df_monthly.columns:\n",
    "        df_monthly[\"date\"] = pd.to_datetime(df_monthly[\"year\"].astype(str) + \"-\" + df_monthly[\"month\"].astype(str) + \"-01\", errors=\"coerce\")\n",
    "    else:\n",
    "        raise SystemExit(\"Monthly CSV lacks 'date' and no year/month columns — cannot proceed. / CSV mensal não tem 'date' nem year/month — não é possível prosseguir.\")\n",
    "\n",
    "# Set index\n",
    "df_monthly = df_monthly.sort_values(\"date\").set_index(\"date\")\n",
    "series = df_monthly[\"cases\"].astype(float)\n",
    "\n",
    "# Basic stats\n",
    "stats = series.describe().to_frame().T\n",
    "stats[\"cv\"] = stats[\"std\"] / stats[\"mean\"]\n",
    "stats = stats[[\"count\",\"mean\",\"std\",\"min\",\"25%\",\"50%\",\"75%\",\"max\",\"cv\"]]\n",
    "stats = stats.rename(columns={\"50%\":\"median\"})\n",
    "print_section(\"Monthly series descriptive statistics:\", \"Estatísticas descritivas da série mensal:\")\n",
    "display(stats.T)\n",
    "\n",
    "# Save table\n",
    "stats.to_csv(TAB_DIR / \"monthly_series_descriptive_stats.csv\", index=False)\n",
    "debug(\"Saved monthly descriptive stats table.\", \"Estatísticas mensais salvas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34cc186",
   "metadata": {},
   "source": [
    "### Step 5: **Data visualization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcd1eac",
   "metadata": {},
   "source": [
    "Calculate and plot the **time series, from 2001-jan to 2010-dec, for cases/month** data visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d19c7f",
   "metadata": {},
   "source": [
    "Related questions:\n",
    "- What is the overall **behavior** of the data over time?\n",
    "- Is there any obvious **trend** (growth/decline) or **seasonality** visible?\n",
    "- Where are the most apparent **outliers**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2017d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EN: Plot monthly time series (line) and save figure\n",
    "# PT-BR: Plot da série mensal (linha) e salvar figura\n",
    "\n",
    "locale.setlocale(locale.LC_TIME, \"en_US.UTF-8\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(25,6))\n",
    "\n",
    "# --- 1. Plotting and Axis Limits ---\n",
    "ax.plot(series.index, series.values, marker='o', linestyle='-', markersize=4)\n",
    "\n",
    "ax.set_title(\"Monthly Pulmonary Tuberculosis cases (time series)\")\n",
    "# ax.set_xlabel is omitted since custom year labels are created below.\n",
    "ax.set_ylabel(\"Cases\")\n",
    "\n",
    "# Sets X-axis limits with a small padding (15 days) before the first and after the last data point.\n",
    "data_inicio_com_margem = series.index[0] - timedelta(days=15)\n",
    "data_fim_com_margem = series.index[-1] + timedelta(days=15)\n",
    "ax.set_xlim(data_inicio_com_margem, data_fim_com_margem)\n",
    "\n",
    "# --- 2. Month Labels (Major Ticks) ---\n",
    "# Locates a major tick at the start of every month.\n",
    "ax.xaxis.set_major_locator(mdates.MonthLocator(interval=1))\n",
    "# Formats the major tick to show the abbreviated month name (Jan, Feb...).\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%b'))\n",
    "# Rotates the month labels for better readability.\n",
    "plt.xticks(rotation=90, fontsize=10)\n",
    "\n",
    "\n",
    "# --- 3. Year Demarcation (Thick Grid Lines) ---\n",
    "years = np.unique(series.index.year)\n",
    "# Loop through each year to draw the vertical separation lines.\n",
    "for y in years:\n",
    "    start_of_year = datetime(y, 1, 1)\n",
    "    # Draws a thick vertical line at the beginning of each year.\n",
    "    ax.axvline(start_of_year, color='black', linewidth=2, zorder=0)\n",
    "\n",
    "\n",
    "# --- 4. Centered Year Labels ---\n",
    "ymin, ymax = ax.get_ylim()\n",
    "# Defines the vertical position for the year label (7% below the minimum Y-axis value).\n",
    "y_text = ymin - (ymax - ymin) * 0.07\n",
    "\n",
    "for y in years:\n",
    "    start = datetime(y, 1, 1)\n",
    "    # Defines the end of the current year block (start of the next year or end of series).\n",
    "    if y == years[-1]:\n",
    "        # For the last year, define the end based on the last data point plus padding.\n",
    "        end = series.index[-1] + timedelta(days=16)\n",
    "    else:\n",
    "        end = datetime(y+1, 1, 1)\n",
    "        \n",
    "    # Calculates the horizontal center point of the year block.\n",
    "    center = start + (end - start) / 2\n",
    "    \n",
    "    # Places the year label centered horizontally in the block.\n",
    "    # The \"\\n\" adds a line break below the month labels.\n",
    "    ax.text(center, y_text, str(\"\\n\" + str(y)), ha='center', va='top', fontsize=12)\n",
    "\n",
    "# --- 5. Grid and Final Layout Adjustments ---\n",
    "# Draws the general grid lines (using major ticks as reference).\n",
    "ax.grid(True, which='major', linestyle='-', linewidth=0.5)\n",
    "\n",
    "# Draws thinner grid lines for all years (this is a redundant step as axvline covers it).\n",
    "for y in years:\n",
    "    start_of_year = datetime(y, 1, 1)\n",
    "    # Draws a simple vertical line for general grid (optional, as thick lines are already in place).\n",
    "    ax.axvline(start_of_year, color='grey', linewidth=0.5)\n",
    "\n",
    "# Adjusts the bottom margin to accommodate the rotated month labels and the centered year labels.\n",
    "plt.subplots_adjust(bottom=0.18)\n",
    "\n",
    "# --- 6. Saving and Display ---\n",
    "fig_path = FIG_DIR / \"ts_monthly_cases_line_excel_style.png\"\n",
    "fig.savefig(fig_path, bbox_inches=\"tight\", dpi=150)\n",
    "print(f\"Saved time series plot to {fig_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c6b80a",
   "metadata": {},
   "source": [
    "Generate **boxplots**, from jan to dec yearly, for **seasonality investigation**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208f018c",
   "metadata": {},
   "source": [
    "Related questions:\n",
    "- Does the series have a recurring **seasonal pattern**?\n",
    "- What is the **dispersion and median** of cases in each month of the year?\n",
    "- Where are the largest **outliers** relative to each seasonal period?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e5fb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EN: Boxplot by month to spot seasonality (month-of-year)\n",
    "# PT-BR: Boxplot por mês para identificar sazonalidade (mês do ano)\n",
    "\n",
    "df_monthly_plot = df_monthly.reset_index()\n",
    "\n",
    "# Extracts the numeric month (1 to 12) from the date column for plotting purposes.\n",
    "df_monthly_plot[\"month_num\"] = df_monthly_plot[\"date\"].dt.month\n",
    "# Defines the explicit order for the months (1 through 12) to ensure chronological display.\n",
    "month_order = list(range(1,13))\n",
    "\n",
    "# Generates the boxplot: X-axis is the month number, Y-axis is the case count.\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.boxplot(x=\"month_num\", y=\"cases\", data=df_monthly_plot, order=month_order)\n",
    "plt.title(\"Monthly distribution of cases by month-of-year\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Cases\")\n",
    "plt.xticks(ticks=np.arange(12), labels=[\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"])\n",
    "\n",
    "# Saves the figure to the specified file path.\n",
    "box_path = FIG_DIR / \"boxplot_month_by_month.png\"\n",
    "plt.savefig(box_path, bbox_inches=\"tight\", dpi=150)\n",
    "debug(f\"Saved month-wise boxplot to {box_path}\", f\"Boxplot mensal salvo em {box_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea146470",
   "metadata": {},
   "source": [
    "Generate **heatmaps** (month x year), from 2001-jan to 2010-dec, for **pattern visualization**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8f3fa8",
   "metadata": {},
   "source": [
    "Related questions:\n",
    "- How is the series' **value distributed** across the Month vs. Year axes?\n",
    "- Is the **seasonality consistent** over the years, or has it intensified/softened over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b878ad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EN: Heatmap (years x months) to visualize patterns across years and months\n",
    "# PT-BR: Heatmap (anos x meses) para visualizar padrões ao longo dos anos e meses\n",
    "\n",
    "# --- 1. Data Preparation (Pivot Table) ---\n",
    "# Creates a pivot table to structure the data for the heatmap.\n",
    "# Index (rows) are the years, columns are the months, and values are the total 'cases'.\n",
    "pivot = df_monthly_plot.pivot_table(index=df_monthly_plot[\"date\"].dt.year, \n",
    "                                    columns=df_monthly_plot[\"date\"].dt.month, \n",
    "                                    values=\"cases\", \n",
    "                                    aggfunc=\"sum\")\n",
    "# Reorders the columns to ensure they are sorted chronologically (Jan to Dec).\n",
    "pivot = pivot.reindex(index=sorted(pivot.index), columns=month_order)\n",
    "\n",
    "# --- 2. Heatmap Generation ---\n",
    "plt.figure(figsize=(12,6))\n",
    "# Generates the heatmap using the pivot table.\n",
    "sns.heatmap(pivot, \n",
    "            annot=True,              # Annotate each cell with its value.\n",
    "            fmt=\".0f\",               # Format the annotation as integers (no decimals).\n",
    "            cmap=\"YlOrRd\",           # Sets the color map (Yellow-Orange-Red, suitable for intensity).\n",
    "            cbar_kws={'label':'cases'}) # Labels the color bar.\n",
    "            \n",
    "# Plot\n",
    "plt.title(\"Heatmap: Year x Month cases\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Year\")\n",
    "plt.xticks(ticks=np.arange(12), labels=[\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"])\n",
    "\n",
    "# --- 3. Saving and Output ---\n",
    "heat_path = FIG_DIR / \"heatmap_year_month_cases.png\"\n",
    "plt.savefig(heat_path, bbox_inches=\"tight\", dpi=150)\n",
    "debug(f\"Saved heatmap to {heat_path}\", f\"Heatmap salvo em {heat_path}\")\n",
    "plt.show()\n",
    "pivot.to_csv(TAB_DIR / \"pivot_year_month_cases.csv\")\n",
    "debug(\"Saved pivot table year-month to tables.\", \"Pivot ano-mes salvo em tables.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af541972",
   "metadata": {},
   "source": [
    "Calculate and **plot the histogram for frequency distribution** visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b65ad8",
   "metadata": {},
   "source": [
    "Related questions:\n",
    "- What is the **frequency distribution** of the values in the series?\n",
    "- Does the data follow a **normal distribution** (bell curve)?\n",
    "- What is the **smooth shape** of the frequency distribution?\n",
    "- What is the most likely value (**mode**) of the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb7bae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EN: Histogram and KDE for monthly counts\n",
    "# PT-BR: Histograma e KDE para contagem mensal\n",
    "\n",
    "# Generates the histogram plot.\n",
    "# 'series.dropna()' ensures only valid case counts are used.\n",
    "# 'kde=True' adds the Kernel Density Estimate line to show the smooth distribution shape.\n",
    "# 'bins=30' specifies that the data range should be divided into 30 intervals (bars).\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.histplot(series.dropna(), kde=True, bins=30)\n",
    "plt.title(\"Histogram of monthly cases\")\n",
    "plt.xlabel(\"Cases\")\n",
    "\n",
    "# Saves the figure. 'bbox_inches=\"tight\"' prevents labels from being cut off.\n",
    "hist_path = FIG_DIR / \"histogram_monthly_cases.png\"\n",
    "plt.savefig(hist_path, bbox_inches=\"tight\", dpi=150)\n",
    "debug(f\"Saved histogram to {hist_path}\", f\"Histograma salvo em {hist_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56985e8f",
   "metadata": {},
   "source": [
    "### Step 6: **Data Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff608c2",
   "metadata": {},
   "source": [
    "**Decomposes the time series** to analyze the data's fundamental structure and **verify the presence of trend and seasonal patterns**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779d6b3d",
   "metadata": {},
   "source": [
    "Related questions:\n",
    "- What is the **underlying level** of the series, excluding fluctuations (Trend)?\n",
    "- What is the exact, **repetitive variation** that occurs in each cycle (Seasonality)?\n",
    "- What **remains** in the series after removing the Trend and Seasonality (Residual)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e71ee0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EN: STL decomposition (trend, seasonal, resid). Period=12 for monthly seasonality.\n",
    "# PT-BR: Decomposição STL (tendência, sazonal, resíduo). Período=12 para sazonalidade mensal.\n",
    "\n",
    "try:\n",
    "    # --- Time Series Decomposition ---\n",
    "    # Breaks down the time series into its core components (Trend, Seasonality, and Residual)\n",
    "    # to understand the underlying structure of the data.\n",
    "    decomposition = seasonal_decompose(series.dropna(), \n",
    "                                       model='additive',       # Assumes components are summed: Y = Trend + Seasonality + Residual.\n",
    "                                       period=12,              # Sets the seasonal cycle length to 12 (as data is monthly, this represents the annual cycle).\n",
    "                                       extrapolate_trend='freq') # Extrapolates the trend line to the edges of the series.\n",
    "    \n",
    "    # --- Plotting ---\n",
    "    fig = decomposition.plot()\n",
    "    fig.set_size_inches(12,8)\n",
    "    plt.xlabel(\"\\nYear\")\n",
    "\n",
    "    # --- Saving ---\n",
    "    stl_path = FIG_DIR / \"stl_decomposition.png\"\n",
    "    fig.savefig(stl_path, bbox_inches=\"tight\", dpi=150)\n",
    "    debug(f\"Saved STL decomposition to {stl_path}\", f\"Decomposição STL salva em {stl_path}\")\n",
    "    plt.show()\n",
    "# --- Error Handling ---\n",
    "except Exception as e:\n",
    "    # Analytical Note: The decomposition fails if there are not enough data points (less than two full cycles) \n",
    "    # or if the data still contains NaN values, leading to an exception.\n",
    "    debug(f\"STL decomposition failed: {e}\", f\"Decomposição STL falhou: {e}\")\n",
    "    print(\"STL decomposition failed — likely too few data points or NaNs. / Falha na decomposição STL — provavelmente poucos pontos ou NaNs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdca64e",
   "metadata": {},
   "source": [
    "Calculates and **plots the Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACP)** of the time series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fe9ff8",
   "metadata": {},
   "source": [
    "Related questions:\n",
    "- Does the series have a **correlation with its own past** values?\n",
    "- What is the **total reach** (direct and indirect) of this correlation?\n",
    "- What is the direct, **pure correlation** between the current value and a specific lag, isolating the influence of intermediate lags?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97787c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EN: Plot ACF and PACF to inspect autocorrelation and help choose ARIMA orders\n",
    "# PT-BR: Plot ACF e PACF para inspecionar autocorrelação e ajudar a escolher ordens ARIMA\n",
    "\n",
    "# --- 1. Autocorrelation Function (ACF) Plot ---\n",
    "plt.figure(figsize=(12,4))\n",
    "# Generates the ACF plot: measures the total correlation between the series (Yt) and its past values (Yt-k).\n",
    "# series.dropna(): Ensures clean data is used for correlation calculation.\n",
    "# lags=36: Analyzes correlation up to 36 months (3 years) into the past.\n",
    "# Purpose: The plot visually identifies the correlation structure of the series, indicating the order of the \n",
    "# Moving Average (MA) component, or 'q', in an ARIMA model (where the ACF cuts off or decays).\n",
    "plot_acf(series.dropna(), lags=36)\n",
    "plt.tight_layout()\n",
    "plt.xlabel(\"Lag (months)\")\n",
    "acf_path = FIG_DIR / \"acf_monthly.png\"\n",
    "plt.savefig(acf_path, bbox_inches=\"tight\", dpi=150)\n",
    "debug(f\"Saved ACF plot to {acf_path}\", f\"ACF salvo em {acf_path}\")\n",
    "plt.show()\n",
    "\n",
    "# --- 2. Partial Autocorrelation Function (PACF) Plot ---\n",
    "plt.figure(figsize=(12,4))\n",
    "# Generates the PACF plot: measures the pure, direct correlation between the series (Yt) and a specific lag (Yt-k), \n",
    "# after removing the influence of all intermediate lags (Yt-1, ..., Yt-k+1).\n",
    "# lags=36: Analyzes direct correlation up to 36 months into the past.\n",
    "# method='ywm': Uses the Yule-Walker method for PACF calculation, which is robust.\n",
    "# Purpose: The plot is used to identify the order of the Autoregressive (AR) component, or 'p', in an ARIMA model \n",
    "# (where the PACF cuts off or decays).\n",
    "plot_pacf(series.dropna(), lags=36, method='ywm')\n",
    "plt.tight_layout()\n",
    "plt.xlabel(\"Lag (months)\")\n",
    "pacf_path = FIG_DIR / \"pacf_monthly.png\"\n",
    "plt.savefig(pacf_path, bbox_inches=\"tight\", dpi=150)\n",
    "debug(f\"Saved PACF plot to {pacf_path}\", f\"PACF salvo em {pacf_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f167db89",
   "metadata": {},
   "source": [
    "**Aggregates the monthly data** into a yearly summary table to calculate the percentage change, allowing for analysis of long-term trends and growth/decline rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b112cc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EN: Yearly aggregated table and year-on-year percent change\n",
    "# PT-BR: Tabela agregada anual e variação percentual ano-a-ano\n",
    "\n",
    "if \"cases\" in df_annual.columns:\n",
    "    df_annual_sorted = df_annual.sort_values(\"year\").copy()\n",
    "else:\n",
    "    # fallback: aggregate from monthly\n",
    "    df_annual_sorted = df_monthly_plot.groupby(df_monthly_plot[\"date\"].dt.year)[\"cases\"].sum().reset_index().rename(columns={\"date\":\"year\"})\n",
    "\n",
    "df_annual_sorted[\"pct_change\"] = df_annual_sorted[\"cases\"].pct_change() * 100\n",
    "display(df_annual_sorted)\n",
    "df_annual_sorted.to_csv(TAB_DIR / \"annual_summary_cases_pctchange.csv\", index=False)\n",
    "debug(\"Saved annual summary table.\", \"Resumo anual salvo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1685272",
   "metadata": {},
   "source": [
    "### EDA Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8784861f",
   "metadata": {},
   "source": [
    "**Saves the EDA results** for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81571943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EN: Save compact EDA summary for later use (quick tables)\n",
    "# PT-BR: Salvar um resumo EDA compacto para uso posterior\n",
    "\n",
    "# Monthly seasonal means\n",
    "seasonal_means = df_monthly_plot.groupby(df_monthly_plot[\"date\"].dt.month)[\"cases\"].mean().rename_axis(\"month\").reset_index()\n",
    "seasonal_means[\"month_name\"] = [\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"]\n",
    "seasonal_means.to_csv(TAB_DIR / \"monthly_seasonal_means.csv\", index=False)\n",
    "\n",
    "# Top months (highest counts)\n",
    "top_months = df_monthly_plot.sort_values(\"cases\", ascending=False).head(20)\n",
    "top_months.to_csv(TAB_DIR / \"top_20_months_by_cases.csv\", index=False)\n",
    "\n",
    "debug(\"Saved seasonal means and top months tables.\", \"Salvo médias sazonais e top meses.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2261030",
   "metadata": {},
   "source": [
    "**List** the generated files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5887f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EN: Optional - list generated files\n",
    "# PT-BR: Opcional - listar arquivos gerados\n",
    "\n",
    "print(\"Figures saved to:\", FIG_DIR.resolve())\n",
    "print(\"Tables saved to:\", TAB_DIR.resolve())\n",
    "print(\"Files in figures (example):\", [p.name for p in sorted(FIG_DIR.glob(\"*.png\"))][:10])\n",
    "print(\"Files in tables (example):\", [p.name for p in sorted(TAB_DIR.glob(\"*.csv\"))][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efb77f8",
   "metadata": {},
   "source": [
    "# Modeling and Forecast\n",
    "Time series diagnostics, SARIMA/ETS model comparison, and forecast (train: 2001-2009, test: 2010, forecast: 2011). Builds candidate time-series models for the monthly pulmonary tuberculosis series, performs backtesting on 2010, compares metrics, and produces a final forecast for 2011 with prediction intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840d50b3",
   "metadata": {},
   "source": [
    "### Step 1: **Stationarity Testing**\n",
    "**Checks if the time series is stationary** by performing the ADF (Augmented Dickey-Fuller Test) to verify for a unit root, and the KPSS (Kwiatkowski-Phillips-Schmidt-Shin Test) to verify stationarity around a mean level or deterministic trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e18071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EN: Run ADF and KPSS tests to check stationarity\n",
    "# PT-BR: Executa testes ADF e KPSS para verificar estacionariedade\n",
    "\n",
    "# Define ADF test function\n",
    "def adf_test(x):\n",
    "    # adfuller: Performs the Augmented Dickey-Fuller test. \n",
    "    # autolag='AIC': Selects the optimal lag length based on the Akaike Information Criterion.\n",
    "    r = adfuller(x.dropna(), autolag='AIC')\n",
    "    # Returns the test statistics, p-value (key for decision), used lag, and number of observations.\n",
    "    return {\"adf_stat\": r[0], \"pvalue\": r[1], \"usedlag\": r[2], \"nobs\": r[3]}\n",
    "\n",
    "# Define KPSS test function\n",
    "def kpss_test(x):\n",
    "    # kpss: Performs the Kwiatkowski-Phillips-Schmidt-Shin test.\n",
    "    # nlags=\"auto\": Automatically determines the number of lags for the test.\n",
    "    r = kpss(x.dropna(), nlags=\"auto\")\n",
    "    # Returns the test statistics, p-value, and number of lags used.\n",
    "    return {\"kpss_stat\": r[0], \"pvalue\": r[1], \"nlags\": r[2]}\n",
    "\n",
    "# Executes the tests on the series data.\n",
    "adf_res = adf_test(series)\n",
    "kpss_res = kpss_test(series)\n",
    "\n",
    "debug(f\"ADF: {adf_res}\", f\"ADF: {adf_res}\")\n",
    "debug(f\"KPSS: {kpss_res}\", f\"KPSS: {kpss_res}\")\n",
    "\n",
    "# Prints the p-values for direct interpretation.\n",
    "print(\"ADF p-value (stationary if < 0.05):\", adf_res[\"pvalue\"])\n",
    "print(\"KPSS p-value (stationary if > 0.05):\", kpss_res[\"pvalue\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905d703d",
   "metadata": {},
   "source": [
    "### Step 2: **Set Definition**\n",
    "Defines the training set, for modeling, and the testing set, for forecasting performance evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839fe597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EN: Train/test split: train <= 2009-12-01, test = 2010-01..2010-12\n",
    "# PT-BR: Divisão Treino/Teste: treino <= 01/Dez/2009, teste = Jan/2010 até Dez/2010\n",
    "\n",
    "# Define the end date for the training set (inclusive).\n",
    "train_end = pd.to_datetime(\"2009-12-01\")\n",
    "# Define the start date for the test set.\n",
    "test_start = pd.to_datetime(\"2010-01-01\")\n",
    "# Define the end date for the test set (inclusive).\n",
    "test_end = pd.to_datetime(\"2010-12-01\")\n",
    "\n",
    "# Creates the training set: all observations up to and including the train_end date.\n",
    "train = series[series.index <= train_end].copy()\n",
    "# Creates the test set: all observations between test_start and test_end (inclusive).\n",
    "test = series[(series.index >= test_start) & (series.index <= test_end)].copy()\n",
    "\n",
    "# Prints the number of data points in each set for verification.\n",
    "debug(f\"Train points: {len(train)}, Test points: {len(test)}\", f\"Treino: {len(train)}, Teste: {len(test)}\")\n",
    "# Displays the last 6 observations of the training set to check the cutoff date.\n",
    "display(train.tail(6))\n",
    "# Displays the first 6 observations of the test set to check the starting date.\n",
    "display(test.head(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3225d34b",
   "metadata": {},
   "source": [
    "### Step 3: **Evaluation Metrics**\n",
    "Implements three metrics to quantify the forecasting models and regression error:\n",
    "- MAE (Mean Absolute Error)\n",
    "- RMSE (Root Mean Squared Error)\n",
    "- MAPE (Mean Absolute Percentage Error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef36812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EN: Define evaluation metrics\n",
    "#PT-BR: Define métricas de avaliação\n",
    "\n",
    "# --- 1. Mean Absolute Error (MAE) ---\n",
    "def mae(y_true, y_pred):\n",
    "    # Calculates the average of the absolute errors.\n",
    "    return mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "# --- 2. Root Mean Squared Error (RMSE) ---\n",
    "def rmse(y_true, y_pred):\n",
    "    # Calculates the square root of the Mean Squared Error (MSE), returning the error to the original unit.\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# --- 3. Mean Absolute Percentage Error (MAPE) ---\n",
    "def mape(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    \n",
    "    # CRITICAL FIX: Creates a mask to exclude zero values in y_true, preventing division by zero.\n",
    "    mask = y_true != 0\n",
    "    \n",
    "    # Handles the edge case where all true values are zero.\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "        \n",
    "    # Calculates the mean of the absolute percentage errors (only for non-zero true values), multiplied by 100.\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52968f79",
   "metadata": {},
   "source": [
    "### Step 4: **Modeling strategy and Training**\n",
    "We'll fit two families of models:\n",
    "- SARIMA via statsmodels (seasonal order with s=12). We'll perform a small grid search over (p,d,q) x (P,D,Q) limited ranges to keep runtime reasonable.\n",
    "- ETS (Holt-Winters) via statsmodels' ExponentialSmoothing as a benchmark.\n",
    "We will evaluate on the 2010 holdout and choose the best model by RMSE/MAE/MAPE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28017a32",
   "metadata": {},
   "source": [
    "#### SARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab2eb08",
   "metadata": {},
   "source": [
    "**Hyperparameters optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6febb816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EN: SARIMA grid search (limited) — careful with runtime\n",
    "# PT-BR: Busca em grade SARIMA (limitada) — atenção ao tempo de execução\n",
    "\n",
    "# Sets random seed for reproducibility.\n",
    "np.random.seed(42)\n",
    "\n",
    "# Defines the search ranges for the non-seasonal (p, d, q) and seasonal (P, D, Q) orders.\n",
    "ps = [0,1,2]\n",
    "ds = [0,1]\n",
    "qs = [0,1,2]\n",
    "Ps = [0,1]\n",
    "Ds = [0,1]\n",
    "Qs = [0,1]\n",
    "s = 12 # Sets the seasonal period to 12 (months).\n",
    "\n",
    "best_sarima = None\n",
    "results = []\n",
    "start_time = time.time()\n",
    "\n",
    "# --- Grid Search Loop ---\n",
    "# We'll fit SARIMA on TRAIN and forecast the 12 months of TEST, compute RMSE\n",
    "# Iterates through all combinations of non-seasonal parameters (p, d, q).\n",
    "for p,d,q in itertools.product(ps,ds,qs):\n",
    "    # Iterates through all combinations of seasonal parameters (P, D, Q).\n",
    "    for P,D,Q in itertools.product(Ps,Ds,Qs):\n",
    "        order = (p,d,q)\n",
    "        seasonal_order = (P,D,Q,s)\n",
    "        try:\n",
    "            # Initializes the SARIMAX model using the current parameter combination.\n",
    "            # train: The model is fitted only on the training data.\n",
    "            # enforce_stationarity/invertibility=False: Allows the fitting of a wider range of models.\n",
    "            model = SARIMAX(train, order=order, seasonal_order=seasonal_order, enforce_stationarity=False, enforce_invertibility=False)\n",
    "            # Fits the model (disp=False suppresses optimization output).\n",
    "            res = model.fit(disp=False, method=\"lbfgs\", maxiter=500)\n",
    "            \n",
    "            # --- Forecasting and Evaluation ---\n",
    "            # Predicts the values for the test period (12 months).\n",
    "            pred = res.get_forecast(steps=len(test))\n",
    "            yhat = pred.predicted_mean\n",
    "            \n",
    "            # Computes evaluation metrics by comparing predictions (yhat) with real values (test).\n",
    "            score_rmse = rmse(test.values, yhat.values)\n",
    "            score_mae = mae(test.values, yhat.values)\n",
    "            score_mape = mape(test.values, yhat.values)\n",
    "            \n",
    "            # Stores the parameters and scores.\n",
    "            results.append({\"order\":order, \"seasonal_order\":seasonal_order, \"rmse\":score_rmse, \"mae\":score_mae, \"mape\":score_mape, \"aic\":res.aic})\n",
    "            debug(f\"SARIMA tried order={order} seasonal={seasonal_order} RMSE={score_rmse:.2f}\", f\"SARIMA testado order={order} seasonal={seasonal_order} RMSE={score_rmse:.2f}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Skips parameter combinations that fail to converge or result in errors.\n",
    "            debug(f\"SARIMA failed for order={order} seasonal={seasonal_order}: {e}\", f\"SARIMA falhou para order={order} seasonal={seasonal_order}: {e}\")\n",
    "            continue\n",
    "\n",
    "# --- Results Processing ---\n",
    "# Converts the results list into a DataFrame and sorts by RMSE to find the best model.\n",
    "if results:\n",
    "    # Creates and sorts DataFrame, but only if results is not empty.\n",
    "    df_sarima_results = pd.DataFrame(results).sort_values(\"rmse\").reset_index(drop=True)\n",
    "else:\n",
    "    # Handles the empty list case to avoid KeyError.\n",
    "    print(\"No SARIMA models successfully fitted. Creating empty DataFrame.\")\n",
    "    df_sarima_results = pd.DataFrame() # Creates a safe empty DataFrame.\n",
    "\n",
    "# Logs the completion time and the parameters of the best performing model.\n",
    "debug(f\"SARIMA grid search completed in {time.time()-start_time:.1f}s; best candidate: {df_sarima_results.iloc[0].to_dict() if not df_sarima_results.empty else 'none'}\",\n",
    "    f\"Busca SARIMA concluída em {time.time()-start_time:.1f}s; melhor candidato: {df_sarima_results.iloc[0].to_dict() if not df_sarima_results.empty else 'nenhum'}\")\n",
    "\n",
    "# Displays the top 10 models ranked by RMSE.\n",
    "display(df_sarima_results.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ece229",
   "metadata": {},
   "source": [
    "**Best model selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf14297e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EN: Fit the best SARIMA from grid search on full TRAIN data (retrain) and produce forecast for 2011 (12 months)\n",
    "# PT-BR: Ajustar o melhor SARIMA da busca em grade nos dados completos de TREINO (refit) e produzir previsão para 2011 (12 meses)\n",
    "\n",
    "# Checks if the grid search returned any valid candidate models.\n",
    "if df_sarima_results.empty:\n",
    "    debug(\"No SARIMA candidate found — skipping SARIMA\", \"Nenhum candidato SARIMA encontrado — pulando SARIMA\")\n",
    "else:\n",
    "    # --- 1. Model Selection ---\n",
    "    # Retrieves the parameters of the best model (lowest RMSE) from the sorted DataFrame.\n",
    "    best = df_sarima_results.iloc[0]\n",
    "    best_order = tuple(best[\"order\"])\n",
    "    best_seasonal = tuple(best[\"seasonal_order\"])\n",
    "    debug(f\"Refitting best SARIMA order={best_order} seasonal={best_seasonal}\", f\"Refit do melhor SARIMA order={best_order} seasonal={best_seasonal}\")\n",
    "\n",
    "    # --- 2. Final Retraining (Refit) ---\n",
    "    # Initializes the SARIMAX model using the optimal parameters found.\n",
    "    sarima_model = SARIMAX(train, order=best_order, seasonal_order=best_seasonal, enforce_stationarity=False, enforce_invertibility=False)\n",
    "    # Fits (retrains) the model on the full training set. maxiter=500 increases optimization attempts.\n",
    "    sarima_res = sarima_model.fit(disp=False, method=\"lbfgs\", maxiter=500)\n",
    "    \n",
    "    # --- 3. Forecast for Test Period (Validation Plot) ---\n",
    "    # Generates a forecast for the previously evaluated test period (2010).\n",
    "    sarima_pred_test = sarima_res.get_forecast(steps=len(test))\n",
    "    sarima_pred_mean_test = sarima_pred_test.predicted_mean\n",
    "    # Calculates the 95% confidence interval (CI) for the test forecast.\n",
    "    sarima_pred_ci_test = sarima_pred_test.conf_int(alpha=0.05)\n",
    "\n",
    "    # --- 4. Final Forecast (Future Prediction) ---\n",
    "    # Generates the final 12-month forecast for the future period (2011).\n",
    "    sarima_forecast_2011 = sarima_res.get_forecast(steps=12)\n",
    "    sarima_forecast_mean = sarima_forecast_2011.predicted_mean\n",
    "    # Calculates the 95% confidence interval (CI) for the 2011 forecast.\n",
    "    sarima_forecast_ci = sarima_forecast_2011.conf_int(alpha=0.05)\n",
    "\n",
    "    # --- 5. Model Persistence ---\n",
    "    # Saves the final fitted model object using pickle for later use (e.g., deployment or future predictions).\n",
    "    import pickle\n",
    "    with open(DATA_DIR / \"sarima_model.pkl\", \"wb\") as f:\n",
    "        pickle.dump(sarima_res, f)\n",
    "\n",
    "    debug(\"SARIMA model refit and saved to processed_data/sarima_model.pkl\",\n",
    "        \"SARIMA refitado e salvo em processed_data/sarima_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08e103b",
   "metadata": {},
   "source": [
    "#### (ETS/Holt-Winters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031c1fff",
   "metadata": {},
   "source": [
    "**Model selection, training, forecasting, error handling and persistence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1161a421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EN: Fit ETS (Holt-Winters) on TRAIN and forecast\n",
    "# PT-BR: Ajusta ETS (Holt-Winters) no conjunto de TREINO e gera previsão\n",
    "\n",
    "# We'll try additive trend + additive seasonality as baseline; user can tune\n",
    "try:\n",
    "    # Initializes the Exponential Smoothing model (Holt-Winters).\n",
    "    # trend=\"add\" (additive trend): Assumes constant growth/decline.\n",
    "    # seasonal=\"add\" (additive seasonality): Assumes constant seasonal magnitude.\n",
    "    # seasonal_periods=12: Sets the seasonal cycle length to 12 months.\n",
    "    ets = ExponentialSmoothing(train, trend=\"add\", seasonal=\"add\", seasonal_periods=12, initialization_method=\"estimated\")\n",
    "    \n",
    "    # Fits the model by optimizing the smoothing parameters (alpha, beta, gamma).\n",
    "    ets_res = ets.fit(optimized=True)\n",
    "    \n",
    "    # Generates a forecast for the test period (2010) for evaluation comparison.\n",
    "    ets_pred_test = ets_res.forecast(steps=len(test))\n",
    "    \n",
    "    # Generates the final 12-month forecast for the future period (2011).\n",
    "    ets_forecast_2011 = ets_res.forecast(steps=12) \n",
    "    \n",
    "    # Save ETS model\n",
    "    import pickle\n",
    "    # Saves the fitted model object using pickle for later use/persistence.\n",
    "    with open(DATA_DIR / \"ets_model.pkl\", \"wb\") as f:\n",
    "        pickle.dump(ets_res, f)\n",
    "        \n",
    "    debug(\"ETS model trained and saved\", \"ETS treinado e salvo\")\n",
    "except Exception as e:\n",
    "    # Handles potential convergence errors during the fitting process.\n",
    "    debug(f\"ETS fitting failed: {e}\", f\"Falha no ajuste ETS: {e}\")\n",
    "    ets_res = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad994308",
   "metadata": {},
   "source": [
    "#### SARIMA and ETS evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6691427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EN/PT: Evaluate SARIMA and ETS on the 2010 test set\n",
    "# PT-BR: Avalia SARIMA e ETS no conjunto de teste de 2010\n",
    "\n",
    "eval_rows = []\n",
    "# --- Evaluation for SARIMA Model ---\n",
    "if not df_sarima_results.empty:\n",
    "    # Uses the mean forecast generated by the best SARIMA model for the test period.\n",
    "    sarima_yhat = sarima_pred_mean_test\n",
    "    eval_rows.append({\n",
    "        \"model\":\"SARIMA\",\n",
    "        # Calculates RMSE, MAE, and MAPE comparing the forecast (yhat) against the actual test values.\n",
    "        \"rmse\": rmse(test.values, sarima_yhat.values),\n",
    "        \"mae\": mae(test.values, sarima_yhat.values),\n",
    "        \"mape\": mape(test.values, sarima_yhat.values)\n",
    "    })\n",
    "\n",
    "# --- Evaluation for ETS Model ---\n",
    "if ets_res is not None:\n",
    "    # Uses the forecast generated by the fitted ETS model for the test period.\n",
    "    ets_yhat = ets_pred_test\n",
    "    eval_rows.append({\n",
    "        \"model\":\"ETS\",\n",
    "        # Calculates RMSE, MAE, and MAPE for the ETS model.\n",
    "        \"rmse\": rmse(test.values, ets_yhat.values),\n",
    "        \"mae\": mae(test.values, ets_yhat.values),\n",
    "        \"mape\": mape(test.values, ets_yhat.values)\n",
    "    })\n",
    "\n",
    "# --- Final Consolidation ---\n",
    "# Converts the results list into a DataFrame.\n",
    "df_eval = pd.DataFrame(eval_rows).sort_values(\"rmse\").reset_index(drop=True)\n",
    "# Displays the comparison table, with the best model (lowest RMSE) at the top.\n",
    "display(df_eval)\n",
    "# Saves the evaluation results to a CSV file.\n",
    "df_eval.to_csv(DATA_DIR / \"model_evaluation_on_2010.csv\", index=False)\n",
    "debug(\"Saved evaluation table to processed_data/model_evaluation_on_2010.csv\",\n",
    "    \"Avaliação salva em processed_data/model_evaluation_on_2010.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9a50ae",
   "metadata": {},
   "source": [
    "### Step 5: **Results presentation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449e844d",
   "metadata": {},
   "source": [
    "**Plots** the training data followed by testing data and the predictions of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66c63a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EN: Plot test period (2010) actual vs predictions (SARIMA and ETS)\n",
    "# PT-BR: Plota período de teste (2010) observados vs predições (SARIMA e ETS)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "# Plots the historical data (Train set) for context.\n",
    "plt.plot(train.index, train, label=\"Train\", color=\"black\")\n",
    "# Plots the actual observed values for the test period (2010).\n",
    "plt.plot(test.index, test, label=\"Actual 2010\", color=\"black\", linestyle=\"--\", marker='o')\n",
    "\n",
    "# --- Plotting SARIMA Forecast ---\n",
    "if not df_sarima_results.empty:\n",
    "    # Plots the mean forecast generated by the best SARIMA model.\n",
    "    plt.plot(test.index, sarima_pred_mean_test, label=\"SARIMA pred (2010)\", marker='o')\n",
    "    ci = sarima_pred_ci_test\n",
    "    # Adds the 95% Confidence Interval (CI) as a shaded area around the forecast line.\n",
    "    plt.fill_between(test.index, ci.iloc[:,0], ci.iloc[:,1], color='C0', alpha=0.2)\n",
    "\n",
    "# --- Plotting ETS Forecast ---\n",
    "if ets_res is not None:\n",
    "    # Plots the forecast generated by the fitted ETS model.\n",
    "    plt.plot(test.index, ets_pred_test, label=\"ETS pred (2010)\", marker='o')\n",
    "\n",
    "# Sets the title, displays the legend, and adds gridlines for readability.\n",
    "plt.title(\"Model predictions vs actual (2010) / Predições vs observados (2010)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt_path = FIG_DIR / \"model_predictions_vs_actual_2010.png\"\n",
    "# Saves the resulting visualization.\n",
    "plt.savefig(plt_path, bbox_inches=\"tight\", dpi=150)\n",
    "debug(f\"Saved prediction vs actual plot: {plt_path}\", f\"Plot predições vs observados salvo: {plt_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69d1261",
   "metadata": {},
   "source": [
    "Checks if the traning was successful and persists the forecasting in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b34cff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EN: Produce and save final 2011 forecasts (SARIMA and ETS) with intervals where available\n",
    "# PT-BR: Produz e salva as previsões finais de 2011 (SARIMA e ETS) com intervalos onde disponíveis\n",
    "\n",
    "forecasts = []\n",
    "\n",
    "# --- 1. SARIMA Forecast Consolidation ---\n",
    "if not df_sarima_results.empty:\n",
    "    sarima_fc = sarima_forecast_mean\n",
    "    sarima_ci = sarima_forecast_ci\n",
    "    # Creates a DataFrame for the SARIMA forecast, including the 95% Confidence Interval (CI).\n",
    "    sarima_df = pd.DataFrame({\n",
    "        \"date\": pd.date_range(start=\"2011-01-01\", periods=12, freq=\"MS\"),\n",
    "        \"forecast_sarima\": sarima_fc.values,\n",
    "        \"lower_sarima\": sarima_ci.iloc[:,0].values,\n",
    "        \"upper_sarima\": sarima_ci.iloc[:,1].values\n",
    "    })\n",
    "    forecasts.append(sarima_df)\n",
    "\n",
    "# --- 2. ETS Forecast Consolidation ---\n",
    "if ets_res is not None:\n",
    "    ets_fc = ets_forecast_2011\n",
    "    # Creates a DataFrame for the ETS forecast (point forecast only).\n",
    "    ets_df = pd.DataFrame({\n",
    "        \"date\": pd.date_range(start=\"2011-01-01\", periods=12, freq=\"MS\"),\n",
    "        \"forecast_ets\": ets_fc.values\n",
    "    })\n",
    "    forecasts.append(ets_df)\n",
    "\n",
    "# --- 3. Merging and Saving ---\n",
    "# Merges all collected forecast DataFrames into a single comparison table.\n",
    "if forecasts:\n",
    "    df_forecast = forecasts[0]\n",
    "    for f in forecasts[1:]:\n",
    "        df_forecast = df_forecast.merge(f, on=\"date\", how=\"outer\")\n",
    "        \n",
    "    # Saves the final 2011 forecasts to CSV.\n",
    "    df_forecast.to_csv(DATA_DIR / \"forecast_2011_models.csv\", index=False)\n",
    "    debug(\"Saved 2011 forecasts to processed_data/forecast_2011_models.csv\",\n",
    "        \"Previsões 2011 salvas em processed_data/forecast_2011_models.csv\")\n",
    "    display(df_forecast)\n",
    "else:\n",
    "    debug(\"No forecasts produced.\", \"Nenhuma previsão produzida.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc09dbb",
   "metadata": {},
   "source": [
    "**The cherry of the cake**: plots the complete dataset, followed by its projection for 2011 with a shaded area for confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c274a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EN: Plot history (2001-2010) + forecasts for 2011 with intervals\n",
    "# PT-BR: Plota histórico (2001-2010) + previsões para 2011 com intervalos\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "# Plots the entire historical time series data (2001-2010).\n",
    "plt.plot(series.index, series.values, label=\"History (2001-2010)\", color=\"black\")\n",
    "\n",
    "# --- Plotting SARIMA Forecast for 2011 ---\n",
    "if not df_sarima_results.empty:\n",
    "    # Plots the SARIMA point forecast (mean) for the 12 months of 2011.\n",
    "    plt.plot(df_forecast[\"date\"], df_forecast[\"forecast_sarima\"], label=\"SARIMA 2011\", marker='o')\n",
    "    # Adds the 95% Confidence Interval (CI) as a shaded area, representing prediction uncertainty.\n",
    "    plt.fill_between(df_forecast[\"date\"], df_forecast[\"lower_sarima\"], df_forecast[\"upper_sarima\"], color='C0', alpha=0.15)\n",
    "\n",
    "# --- Plotting ETS Forecast for 2011 ---\n",
    "if ets_res is not None:\n",
    "    # Plots the ETS point forecast (mean) for the 12 months of 2011.\n",
    "    plt.plot(df_forecast[\"date\"], df_forecast[\"forecast_ets\"], label=\"ETS 2011\", marker='o')\n",
    "\n",
    "# Sets the title, legend, and grid, finalizing the visualization.\n",
    "plt.title(\"History + Forecast (2011) / Histórico + Previsão (2011)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt_path = FIG_DIR / \"history_plus_forecast_2011.png\"\n",
    "# Saves the final figure.\n",
    "plt.savefig(plt_path, bbox_inches=\"tight\", dpi=150)\n",
    "debug(f\"Saved history + forecast plot: {plt_path}\", f\"Histórico + previsão salvo: {plt_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e512e60",
   "metadata": {},
   "source": [
    "### Modeling Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2ff771",
   "metadata": {},
   "source": [
    "This final step provides a definitive audit of all artifacts generated by the project. It systematically lists the contents of the output directories (processed_data and figures).\n",
    "\n",
    "It serves three main purposes:\n",
    "\n",
    "- Verification: Confirms that all expected outputs (evaluation tables, model files, forecasts, and visualizations) were successfully created and saved.\n",
    "\n",
    "- Organization & Reproducibility: Provides a comprehensive summary of the project's key assets, essential for anyone consuming or reproducing the results.\n",
    "\n",
    "- Quick Location: Helps locate critical files, such as the serialized SARIMA model (.pkl) and the final 2011 forecasts (.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf2b6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EN: Save key outputs and list files\n",
    "# PT-BR: Salva saídas chave e lista os arquivos\n",
    "\n",
    "# Prints the list of saved output files in the processed_data directory.\n",
    "print(\"Saved files in processed_data:\")\n",
    "# Uses glob to find and list all CSV files, sorted alphabetically.\n",
    "for p in sorted(DATA_DIR.glob(\"*.csv\")):\n",
    "    print(\"-\", p.name)\n",
    "# Finds and lists all model files saved in pickle format (.pkl).\n",
    "for p in sorted(DATA_DIR.glob(\"*.pkl\")):\n",
    "    print(\"-\", p.name)\n",
    "\n",
    "# Prints the list of generated figures.\n",
    "print(\"\\nFigures in figures/:\")\n",
    "# Finds and lists all saved PNG image files.\n",
    "for p in sorted(FIG_DIR.glob(\"*.png\")):\n",
    "    print(\"-\", p.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
